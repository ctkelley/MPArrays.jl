var documenterSearchIndex = {"docs":
[{"location":"#MultiPrecisionArrays.jl-v0.0.4","page":"Home","title":"MultiPrecisionArrays.jl v0.0.4","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"C. T. Kelley","category":"page"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionArrays.jl is a package for iterative refinement. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is mostly for research at this point. It will get more interesting as hardware support for Float16 comes online.","category":"page"},{"location":"functions/hlu!/","page":"-","title":"-","text":"#hlu!: Get LU to perform reasonably well for Float16","category":"page"},{"location":"functions/hlu!/","page":"-","title":"-","text":"hlu!(A::Matrix{T}) where{T}","category":"page"},{"location":"functions/hlu!/#MultiPrecisionArrays.hlu!-Union{Tuple{Matrix{T}}, Tuple{T}} where T","page":"-","title":"MultiPrecisionArrays.hlu!","text":"hlu!(A::Matrix{T}) where {T} Return LU factorization of A\n\nC. T. Kelley, 2023\n\nThis function is a hack of generic_lufact! which is part of\n\nhttps://github.com/JuliaLang/julia/blob/master/stdlib/LinearAlgebra/src/lu.jl\n\nI \"fixed\" the code to be Float16 only and fixed pivoting to only MaxRow.\n\nAll I did in the factorization was thread the critical loop with Polyester.@batch and put @simd in the inner loop. These changes got me a 10x speedup on my Mac M2 Pro with 8 performance cores. I'm happy.\n\n\n\n\n\n","category":"method"}]
}
