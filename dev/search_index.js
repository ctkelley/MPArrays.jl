var documenterSearchIndex = {"docs":
[{"location":"functions/mplu!/#mpul!:-Simple-MPArray-factorization","page":"mpul!: Simple MPArray factorization","title":"mpul!: Simple MPArray factorization","text":"","category":"section"},{"location":"functions/mplu!/","page":"mpul!: Simple MPArray factorization","title":"mpul!: Simple MPArray factorization","text":"mplu!(MPA::MPArray)","category":"page"},{"location":"functions/mplu!/#MultiPrecisionArrays.mplu!-Tuple{MPArray}","page":"mpul!: Simple MPArray factorization","title":"MultiPrecisionArrays.mplu!","text":"mplu!(MPA::MPArray)\n\nPlain vanilla MPArray factorization. Economy trianglar solve without interprecision transfers on the fly.\n\n\n\n\n\n","category":"method"},{"location":"#MultiPrecisionArrays.jl-v0.0.4","page":"Home","title":"MultiPrecisionArrays.jl v0.0.4","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"C. T. Kelley","category":"page"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionArrays.jl is a package for iterative refinement. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is mostly for research at this point. It will get more interesting as hardware support for Float16 comes online.","category":"page"},{"location":"functions/hlu!/#hlu!:-Get-LU-to-perform-reasonably-well-for-Float16","page":"hlu!: Get LU to perform reasonably well for Float16","title":"hlu!: Get LU to perform reasonably well for Float16","text":"","category":"section"},{"location":"functions/hlu!/","page":"hlu!: Get LU to perform reasonably well for Float16","title":"hlu!: Get LU to perform reasonably well for Float16","text":"hlu!(A::Matrix{T}) where{T}","category":"page"},{"location":"functions/hlu!/#MultiPrecisionArrays.hlu!-Union{Tuple{Matrix{T}}, Tuple{T}} where T","page":"hlu!: Get LU to perform reasonably well for Float16","title":"MultiPrecisionArrays.hlu!","text":"hlu!(A::Matrix{T}) where {T} Return LU factorization of A\n\nC. T. Kelley, 2023\n\nThis function is a hack of generic_lufact! which is part of\n\nhttps://github.com/JuliaLang/julia/blob/master/stdlib/LinearAlgebra/src/lu.jl\n\nI \"fixed\" the code to be Float16 only and fixed pivoting to only MaxRow.\n\nAll I did in the factorization was thread the critical loop with Polyester.@batch and put @simd in the inner loop. These changes got me a 10x speedup on my Mac M2 Pro with 8 performance cores. I'm happy.\n\n\n\n\n\n","category":"method"}]
}
