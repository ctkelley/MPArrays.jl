<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>More than you want to know · MultiPrecisionArrays.jl</title><meta name="title" content="More than you want to know · MultiPrecisionArrays.jl"/><meta property="og:title" content="More than you want to know · MultiPrecisionArrays.jl"/><meta property="twitter:title" content="More than you want to know · MultiPrecisionArrays.jl"/><meta name="description" content="Documentation for MultiPrecisionArrays.jl."/><meta property="og:description" content="Documentation for MultiPrecisionArrays.jl."/><meta property="twitter:description" content="Documentation for MultiPrecisionArrays.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultiPrecisionArrays.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>More than you want to know</a><ul class="internal"><li><a class="tocitem" href="#Interprecision-Transfers:-Part-I"><span>Interprecision Transfers: Part I</span></a></li><li><a class="tocitem" href="#Half-Precision"><span>Half Precision</span></a></li></ul></li><li><span class="tocitem">MPArray Constructors</span><ul><li><a class="tocitem" href="../functions/MPArray/">MPArray: constructor</a></li></ul></li><li><span class="tocitem">Factorizations</span><ul><li><a class="tocitem" href="../functions/hlu!/">hlu!: Get LU to perform reasonably well for Float16</a></li><li><a class="tocitem" href="../functions/mplu!/">mplu!: Simple MPArray factorization</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../functions/mpgeslir/">mpgeslir: IR solver</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>More than you want to know</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>More than you want to know</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ctkelley/MultiPrecisionArrays.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ctkelley/MultiPrecisionArrays.jl/blob/main/docs/src/Details.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="More-than-you-want-to-know"><a class="docs-heading-anchor" href="#More-than-you-want-to-know">More than you want to know</a><a id="More-than-you-want-to-know-1"></a><a class="docs-heading-anchor-permalink" href="#More-than-you-want-to-know" title="Permalink"></a></h1><h2 id="Interprecision-Transfers:-Part-I"><a class="docs-heading-anchor" href="#Interprecision-Transfers:-Part-I">Interprecision Transfers: Part I</a><a id="Interprecision-Transfers:-Part-I-1"></a><a class="docs-heading-anchor-permalink" href="#Interprecision-Transfers:-Part-I" title="Permalink"></a></h2><p>The discussion in this section is for the most useful case where high precision is <code>Float64</code> and low precision is <code>Float32</code>. Things are different if low precision is <code>Float16</code>.</p><p>Recall that the default way to use the low precision factorization  is to copy <span>$r$</span> into low precision, scale it, perform the solve in  low precision, and then reverse the scaling and promote the  correction <span>$d$</span>. So if <span>$AF = lu!(A32)$</span> is  the factorization object for the low precision factorization, then we compute <span>$d$</span> via</p><pre><code class="nohighlight hljs">d = norm(r)* Float64.( AF\ (Float32.(r / norm(r))))</code></pre><p>We will refer to this approach as the low precision solve (LPS).  As we said earlier, if one simply does</p><pre><code class="nohighlight hljs">d = AF\r</code></pre><p>the elements of the triangular matrices are promoted to double as the solves take place. We will refer to this as a mixed precision solve (MPS). In the table below we report timings from Julia&#39;s  <strong>BenchmarkTools</strong> package for double precision matrix vector multiply (MV64), single precision LU factorization (LU32) and three approaches for using the factors to solve a linear system. HPS is the time for a fully double precision triangular solved and MPS and LPS are the mixed precision solve and the fully low precision solve. IR will use a high precision matrix vector multiply to compute the residual and a solve to compute the correction for each iteration. The low precision factorization is done only once.</p><p>In this example <span>$A = I + 800 G(N)$</span> and we look at several values of <span>$N$</span>.</p><pre><code class="nohighlight hljs">    N      MV64       LU32       HPS        MPS        LPS   LU32/MPS
  512    2.8e-05    7.7e-04    5.0e-05    1.0e-04    2.8e-05 7.8e+00
 1024    1.1e-04    2.6e-03    1.9e-04    7.7e-04    1.0e-04 3.4e+00
 2048    6.1e-04    1.4e-02    8.8e-04    3.5e-03    4.0e-04 4.0e+00
 4096    1.9e-03    8.4e-02    4.7e-03    1.4e-02    2.2e-03 5.8e+00
 8192    6.9e-03    5.9e-01    1.9e-02    5.9e-02    9.7e-03 9.9e+00</code></pre><p>The last column of the table is the ratio of timings for the low precision factorization and the mixed precision solve. Keeping in mind that at least two solves will be needed in IR, the table shows that MPS can be a significant fraction of the cost of the solve for smaller problems and that LPS is at least 4 times less costly. This is a compelling case for using LPS in case considered in this section, where high precision is double and low precision is single, provided the performance of IR is equally good.</p><p>If one is solving <span>$\ma \vx = \vb$</span> for multiple right hand sides, as one would do for nonlinear equations in many cases, then LPS is significantly faster for small and moderately large problems. For example, for <span>$N=4096$</span> the cost of MPS is roughly <span>$15\%$</span> of the low precision LU factorization, so if one does more than 6 solves with the same factorization, the solve cost would be more than the factorization cost. LPS is five times faster and we saw this effect while preparing our our nonlinear solver package <strong>SIAMFANL.jl</strong>. The situation for IR is similar, but one must consider the cost of the high precision matrix-vector multiply, which is about the same as LPS.</p><p>We make LPS the default for IR if high precision is double and low precision is single. This decision is good for desktop computing. If low precision is half, then the LPS vs MPS decision needs more scrutiny.</p><p>Since MPS does the triangular solves in high precision, one should expect that the results will be more accurate and that the improved accuracy might enable the IR loop to terminate earlier \cite{CarsonHigham}. We should be able to see that by timing the IR loop after computing the factorization. One should also verify that the residual norms are equally good.</p><p>We will conclude this section with two final tables for the results of IR with <span>$A = I + \alpha G(N)$</span>. We compare the well conditioned case (<span>$\alpha=1$</span>) and the ill-conditioned case (<span>$\alpha=800$</span>) for a few values of <span>$N$</span>. We will look at residual and error norms for both approaches to interprecision transfer. The conclusion is that if high precision is double and low is single, the two approaches give equally good results. </p><p>The columns of the tables are the dimensions, the <span>$\ell^\infty$</span> relative error norms for both LP and MP interprecision transfers (ELP and EMP) and the corresponding relative residual norms (RLP and RMP).</p><p>The results for <span>$\alpha=1$</span> took 5 IR iterations for all cases. As expected the LPS iteration was faster than MPS. However, for the ill-conditioned <span>$\alpha=800$</span> case, MPS took one fewer iteration (5 vs 6) than EPS for all but the smallest problem. Even so, the overall solve times were essentially the same.</p><p class="math-container">\[\alpha=1\]</p><pre><code class="nohighlight hljs">    N      ELP        EMP        RLP         RMP        TLP       TMP 
  512    4.4e-16    5.6e-16    3.9e-16    3.9e-16    2.8e-04   3.6e-04 
 1024    6.7e-16    4.4e-16    3.9e-16    3.9e-16    1.2e-03   1.5e-03 
 2048    5.6e-16    4.4e-16    3.9e-16    3.9e-16    5.8e-03   6.2e-03 
 4096    1.1e-15    1.1e-15    7.9e-16    7.9e-16    1.9e-02   2.4e-02 
 8192    8.9e-16    6.7e-16    7.9e-16    5.9e-16    7.0e-02   8.9e-02 </code></pre><p class="math-container">\[\alpha=800\]</p><pre><code class="nohighlight hljs">    N      ELP        EMP        RLP         RMP        TLP       TMP 
  512    6.3e-13    6.2e-13    2.1e-15    1.8e-15    3.3e-04   3.8e-04 
 1024    9.6e-13    1.1e-12    3.4e-15    4.8e-15    1.3e-03   1.4e-03 
 2048    1.0e-12    1.2e-12    5.1e-15    4.5e-15    7.2e-03   6.8e-03 
 4096    2.1e-12    2.1e-12    6.6e-15    7.5e-15    2.4e-02   2.5e-02 
 8192    3.3e-12    3.2e-12    9.0e-15    1.0e-14    8.4e-02   8.9e-02 </code></pre><h2 id="Half-Precision"><a class="docs-heading-anchor" href="#Half-Precision">Half Precision</a><a id="Half-Precision-1"></a><a class="docs-heading-anchor-permalink" href="#Half-Precision" title="Permalink"></a></h2><p>Using half precision will not speed anything up, in fact it will make  the solver slower. The reason for this is that LAPACK and the BLAS  do not (<strong>YET</strong>) support half precision, so all the clever stuff in there is missing. We provide a half precision LU factorization <strong>/src/Factorizations/hlu!.jl</strong> that is better than nothing.  It&#39;s a hack of Julia&#39;s  <code>generic_lu!</code> with threading and a couple compiler directives. Even so, it&#39;s 2.5 – 5 x <strong>slower</strong> than a  double precision LU. Half precision support is coming  (Julia and Apple support it in hardware!) but for now, at least for desktop computing, half precision is for research in iterative refinement, not applications. </p><p>Here&#39;s a table (created with  <strong>/Code<em>For</em>Docs/HalfTime.jl</strong> ) that illustrates the point. In the table we compare timings for LAPACK&#39;s LU to the LU we compute with <code>hlu!.jl</code>. The matrix is  <span>$I-800.0*G$</span>.</p><pre><code class="nohighlight hljs">      N       F64       F32       F16     F16/F64 
     1024  3.65e-03  2.65e-03  5.26e-03  1.44e+00 
     2048  2.26e-02  1.41e-02  3.70e-02  1.64e+00 
     4096  1.55e-01  8.53e-02  2.55e-01  1.65e+00 
     8192  1.15e+00  6.05e-01  4.23e+00  3.69e+00 </code></pre><p>The columns of the table are the dimension of the problem, timings for double, single, and half precision, and the ratio of the half precision timings to double. The timings came from Julia 1.10-beta2 running on an Apple M2 Pro with 8 performance cores.</p><p>Half precision is also difficult to use properly. The low precision can  make iterative refinement fail because the half precision factorization  can have a large error. Here is an example to illustrate this point.  The matrix here is modestly ill-conditioned and you can see that in the  error from a direct solve in double precision.</p><pre><code class="nohighlight hljs">julia&gt; A=I - 800.0*G;

julia&gt; x=ones(N);

julia&gt; b=A*x;

julia&gt; xd=A\b;

julia&gt; norm(b-A*xd,Inf)
6.96332e-13

julia&gt; norm(xd-x,Inf)
2.30371e-12</code></pre><p>Now, if we downcast things to half precision, nothing good happens.</p><pre><code class="nohighlight hljs">julia&gt; AH=Float16.(A);

julia&gt; AHF=hlu!(AH);

julia&gt; z=AHF\b;

julia&gt; norm(b-A*z,Inf)
6.25650e-01

julia&gt; norm(z-xd,Inf)
2.34975e-01</code></pre><p>So you get very poor, but unsurprising, results. While <strong>MultiPrecisionArrays.jl</strong> supports half precision and I use it all the time, it is not something you would use in your own work without looking at the literature and making certain you are prepared for strange results. Getting good results consistently from half precision is an active research area.</p><p>First the \alpha ```</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../functions/MPArray/">MPArray: constructor »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.0.0 on <span class="colophon-date" title="Friday 15 September 2023 18:29">Friday 15 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
